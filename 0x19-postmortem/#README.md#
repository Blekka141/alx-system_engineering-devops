### Task 0:
## Issue Summary

**Duration of the Outage:**  
Start: 2024-04-18 09:30 SAST  
End: 2024-04-18 11:00 SAST  
**Impact:**  
Our internal email server experienced an outage, preventing employees from sending or receiving emails. This impacted approximately 90% of our staff, leading to significant communication delays and affecting workflow across multiple departments.  
**Root Cause:**  
A corrupted update to the email server software caused a critical service to fail, leading to the server becoming unresponsive.

### Timeline

- **09:30** - Issue detected through a monitoring alert indicating the email server was unresponsive.
- **09:35** - IT support engineer confirmed the email outage by attempting to access the server.
- **09:40** - Investigation began with checking the server's status and recent logs.
- **09:50** - Initial assumption was a network connectivity issue, leading to checks on network infrastructure.
- **10:00** - Misleading path: Restarted the server's network services, but the issue persisted.
- **10:10** - Checked server hardware for potential failures, but found all hardware components functional.
- **10:20** - Issue escalated to the system administration team.
- **10:30** - Detailed review of recent updates revealed a corrupted update package.
- **10:40** - Reverted the email server to the previous stable version.
- **10:50** - Restarted the email server and tested functionality.
- **11:00** - Service restored and monitored to confirm resolution.

### Root Cause and Resolution

**Root Cause:**  
The root cause was a corrupted update to the email server software. The update package was incomplete, causing a critical service required for the email server to function to fail upon installation. This failure rendered the email server unresponsive.

**Resolution:**  
The issue was resolved by identifying the corrupted update package and reverting the email server to the previous stable version. The system administration team then reinstalled the stable version of the email server software, ensuring all services were operational. The email server was restarted, and normal operations were confirmed.

### Corrective and Preventative Measures

**Improvements/Fixes:**

1. **Implement Update Validation:**  
   Introduce a validation process for all software updates to ensure packages are complete and not corrupted before installation.

2. **Enhance Monitoring:**  
   Improve monitoring systems to include alerts for software update issues and service health checks immediately after updates.

3. **Develop a Rollback Strategy:**  
   Create a robust rollback strategy to quickly revert to previous versions in case of update failures.

**Task List:**

- **Validate Update Packages:** Implement scripts to automatically validate update packages before deployment.
- **Improve Monitoring Tools:** Upgrade monitoring tools to provide real-time feedback on service health post-update.
- **Train IT and SysAdmin Teams:** Conduct training sessions on the new validation and rollback processes.
- **Review Update Policies:** Revise update policies to include mandatory validation checks.
- **Conduct Regular Audits:** Schedule regular audits of the update process to ensure compliance with new policies.

By addressing these measures, we aim to prevent similar incidents in the future and enhance the reliability of our email server.

### Task 1:
The Day the Emails Stood Still 🛑📧
Issue Summary

Duration of the Outage:
Start: 2024-04-18 09:30 SAST
End: 2024-04-18 11:00 SAST

Impact:
Imagine trying to send an urgent email, and nothing happens. This was our reality for 90% of our staff, who suddenly found themselves in a digital silence. Significant communication delays ensued, leading to chaos (and lots of coffee) across multiple departments.

Root Cause:
A corrupted update to our beloved email server software caused a critical service to fail. Our email server decided it needed a break and became unresponsive.
Timeline

    09:30 - 🔔 Issue detected: Monitoring alert says, "Email server is taking a nap."
    09:35 - 🕵️‍♂️ IT support confirms: "Yup, the server's out cold."
    09:40 - 🔍 Investigation begins: Checking server status and logs.
    09:50 - 🛠️ Initial assumption: "Must be a network issue, right?"
    10:00 - 🌐 Misleading path: Restarted network services. "Nope, still napping."
    10:10 - 🔧 Checked hardware: "Everything looks good here."
    10:20 - 🚨 Escalated to sysadmin team.
    10:30 - 📜 Detailed review: "Hey, this update package looks sketchy!"
    10:40 - 🔄 Reverted to previous version: "Back to the old reliable."
    10:50 - 🔄 Restarted email server: "Please wake up."
    11:00 - 🎉 Service restored: "And we’re back online!"

Root Cause and Resolution

Root Cause:
The email server update was corrupted, leading to a failure in a critical service. This was like trying to bake a cake without flour—disastrous and unresponsive.

Resolution:
We identified the bad update, reverted to the previous stable version, and reinstalled the good version. It was like hitting undo on a major typo.
Corrective and Preventative Measures

Improvements/Fixes:

    Implement Update Validation:
    Think of this as the "spell-check" for our updates. No more corrupted packages slipping through.

    Enhance Monitoring:
    Our monitoring now has eyes everywhere, even on those sneaky updates.

    Develop a Rollback Strategy:
    We’ve installed a "panic button" to roll back to safer versions when things go wrong.

Task List:

    Validate Update Packages: Script the spell-check for updates.
    Improve Monitoring Tools: More sensors, more alerts, more peace of mind.
    Train IT and SysAdmin Teams: Training on the new "spell-check" and "panic button."
    Review Update Policies: Policies now require validation checks before hitting "install."
    Conduct Regular Audits: Regular "spell-check" reviews to ensure compliance.

By turning our disaster into a learning opportunity, we aim to prevent future incidents and ensure our email server stays reliable and robust.